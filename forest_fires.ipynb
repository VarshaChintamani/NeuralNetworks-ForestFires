{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "168d5e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in c:\\users\\varsha\\anaconda3\\lib\\site-packages (2.10.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b551ea3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\varsha\\anaconda3\\lib\\site-packages (2.10.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\varsha\\anaconda3\\lib\\site-packages (from tensorflow) (14.0.6)\n",
      "Requirement already satisfied: keras<2.11,>=2.10.0 in c:\\users\\varsha\\anaconda3\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\varsha\\anaconda3\\lib\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\varsha\\anaconda3\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\varsha\\anaconda3\\lib\\site-packages (from tensorflow) (1.3.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\varsha\\anaconda3\\lib\\site-packages (from tensorflow) (3.6.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in c:\\users\\varsha\\anaconda3\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\varsha\\anaconda3\\lib\\site-packages (from tensorflow) (1.21.5)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\users\\varsha\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\varsha\\anaconda3\\lib\\site-packages (from tensorflow) (2.0.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\varsha\\anaconda3\\lib\\site-packages (from tensorflow) (21.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\varsha\\anaconda3\\lib\\site-packages (from tensorflow) (61.2.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\varsha\\anaconda3\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: tensorboard<2.11,>=2.10 in c:\\users\\varsha\\anaconda3\\lib\\site-packages (from tensorflow) (2.10.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\varsha\\anaconda3\\lib\\site-packages (from tensorflow) (1.42.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\varsha\\anaconda3\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\varsha\\anaconda3\\lib\\site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\varsha\\anaconda3\\lib\\site-packages (from tensorflow) (4.1.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\varsha\\anaconda3\\lib\\site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\varsha\\anaconda3\\lib\\site-packages (from tensorflow) (0.27.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\varsha\\anaconda3\\lib\\site-packages (from tensorflow) (3.19.1)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\varsha\\anaconda3\\lib\\site-packages (from tensorflow) (22.10.26)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\varsha\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\varsha\\anaconda3\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\varsha\\anaconda3\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.27.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\varsha\\anaconda3\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\varsha\\anaconda3\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (1.33.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\varsha\\anaconda3\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.0.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\varsha\\anaconda3\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\varsha\\anaconda3\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\varsha\\anaconda3\\lib\\site-packages (from packaging->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\varsha\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\varsha\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\varsha\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (4.2.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\varsha\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\varsha\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\varsha\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\varsha\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\varsha\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (3.3)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\varsha\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\varsha\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow) (3.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3469cf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the libraries\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b780d00c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>FFMC</th>\n",
       "      <th>DMC</th>\n",
       "      <th>DC</th>\n",
       "      <th>ISI</th>\n",
       "      <th>temp</th>\n",
       "      <th>RH</th>\n",
       "      <th>wind</th>\n",
       "      <th>rain</th>\n",
       "      <th>...</th>\n",
       "      <th>monthfeb</th>\n",
       "      <th>monthjan</th>\n",
       "      <th>monthjul</th>\n",
       "      <th>monthjun</th>\n",
       "      <th>monthmar</th>\n",
       "      <th>monthmay</th>\n",
       "      <th>monthnov</th>\n",
       "      <th>monthoct</th>\n",
       "      <th>monthsep</th>\n",
       "      <th>size_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mar</td>\n",
       "      <td>fri</td>\n",
       "      <td>86.2</td>\n",
       "      <td>26.2</td>\n",
       "      <td>94.3</td>\n",
       "      <td>5.1</td>\n",
       "      <td>8.2</td>\n",
       "      <td>51</td>\n",
       "      <td>6.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>oct</td>\n",
       "      <td>tue</td>\n",
       "      <td>90.6</td>\n",
       "      <td>35.4</td>\n",
       "      <td>669.1</td>\n",
       "      <td>6.7</td>\n",
       "      <td>18.0</td>\n",
       "      <td>33</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>oct</td>\n",
       "      <td>sat</td>\n",
       "      <td>90.6</td>\n",
       "      <td>43.7</td>\n",
       "      <td>686.9</td>\n",
       "      <td>6.7</td>\n",
       "      <td>14.6</td>\n",
       "      <td>33</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mar</td>\n",
       "      <td>fri</td>\n",
       "      <td>91.7</td>\n",
       "      <td>33.3</td>\n",
       "      <td>77.5</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.3</td>\n",
       "      <td>97</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mar</td>\n",
       "      <td>sun</td>\n",
       "      <td>89.3</td>\n",
       "      <td>51.3</td>\n",
       "      <td>102.2</td>\n",
       "      <td>9.6</td>\n",
       "      <td>11.4</td>\n",
       "      <td>99</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  month  day  FFMC   DMC     DC  ISI  temp  RH  wind  rain  ...  monthfeb  \\\n",
       "0   mar  fri  86.2  26.2   94.3  5.1   8.2  51   6.7   0.0  ...         0   \n",
       "1   oct  tue  90.6  35.4  669.1  6.7  18.0  33   0.9   0.0  ...         0   \n",
       "2   oct  sat  90.6  43.7  686.9  6.7  14.6  33   1.3   0.0  ...         0   \n",
       "3   mar  fri  91.7  33.3   77.5  9.0   8.3  97   4.0   0.2  ...         0   \n",
       "4   mar  sun  89.3  51.3  102.2  9.6  11.4  99   1.8   0.0  ...         0   \n",
       "\n",
       "   monthjan  monthjul  monthjun  monthmar  monthmay  monthnov  monthoct  \\\n",
       "0         0         0         0         1         0         0         0   \n",
       "1         0         0         0         0         0         0         1   \n",
       "2         0         0         0         0         0         0         1   \n",
       "3         0         0         0         1         0         0         0   \n",
       "4         0         0         0         1         0         0         0   \n",
       "\n",
       "   monthsep  size_category  \n",
       "0         0          small  \n",
       "1         0          small  \n",
       "2         0          small  \n",
       "3         0          small  \n",
       "4         0          small  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read the file\n",
    "fire = pd.read_csv('forestfires.csv')\n",
    "fire.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "534d13cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 517 entries, 0 to 516\n",
      "Data columns (total 31 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   month          517 non-null    object \n",
      " 1   day            517 non-null    object \n",
      " 2   FFMC           517 non-null    float64\n",
      " 3   DMC            517 non-null    float64\n",
      " 4   DC             517 non-null    float64\n",
      " 5   ISI            517 non-null    float64\n",
      " 6   temp           517 non-null    float64\n",
      " 7   RH             517 non-null    int64  \n",
      " 8   wind           517 non-null    float64\n",
      " 9   rain           517 non-null    float64\n",
      " 10  area           517 non-null    float64\n",
      " 11  dayfri         517 non-null    int64  \n",
      " 12  daymon         517 non-null    int64  \n",
      " 13  daysat         517 non-null    int64  \n",
      " 14  daysun         517 non-null    int64  \n",
      " 15  daythu         517 non-null    int64  \n",
      " 16  daytue         517 non-null    int64  \n",
      " 17  daywed         517 non-null    int64  \n",
      " 18  monthapr       517 non-null    int64  \n",
      " 19  monthaug       517 non-null    int64  \n",
      " 20  monthdec       517 non-null    int64  \n",
      " 21  monthfeb       517 non-null    int64  \n",
      " 22  monthjan       517 non-null    int64  \n",
      " 23  monthjul       517 non-null    int64  \n",
      " 24  monthjun       517 non-null    int64  \n",
      " 25  monthmar       517 non-null    int64  \n",
      " 26  monthmay       517 non-null    int64  \n",
      " 27  monthnov       517 non-null    int64  \n",
      " 28  monthoct       517 non-null    int64  \n",
      " 29  monthsep       517 non-null    int64  \n",
      " 30  size_category  517 non-null    object \n",
      "dtypes: float64(8), int64(20), object(3)\n",
      "memory usage: 125.3+ KB\n"
     ]
    }
   ],
   "source": [
    "#EDA\n",
    "fire.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02393eac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FFMC</th>\n",
       "      <th>DMC</th>\n",
       "      <th>DC</th>\n",
       "      <th>ISI</th>\n",
       "      <th>temp</th>\n",
       "      <th>RH</th>\n",
       "      <th>wind</th>\n",
       "      <th>rain</th>\n",
       "      <th>area</th>\n",
       "      <th>dayfri</th>\n",
       "      <th>...</th>\n",
       "      <th>monthdec</th>\n",
       "      <th>monthfeb</th>\n",
       "      <th>monthjan</th>\n",
       "      <th>monthjul</th>\n",
       "      <th>monthjun</th>\n",
       "      <th>monthmar</th>\n",
       "      <th>monthmay</th>\n",
       "      <th>monthnov</th>\n",
       "      <th>monthoct</th>\n",
       "      <th>monthsep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>517.000000</td>\n",
       "      <td>517.000000</td>\n",
       "      <td>517.000000</td>\n",
       "      <td>517.000000</td>\n",
       "      <td>517.000000</td>\n",
       "      <td>517.000000</td>\n",
       "      <td>517.000000</td>\n",
       "      <td>517.000000</td>\n",
       "      <td>517.000000</td>\n",
       "      <td>517.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>517.000000</td>\n",
       "      <td>517.000000</td>\n",
       "      <td>517.000000</td>\n",
       "      <td>517.000000</td>\n",
       "      <td>517.000000</td>\n",
       "      <td>517.000000</td>\n",
       "      <td>517.000000</td>\n",
       "      <td>517.000000</td>\n",
       "      <td>517.000000</td>\n",
       "      <td>517.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>90.644681</td>\n",
       "      <td>110.872340</td>\n",
       "      <td>547.940039</td>\n",
       "      <td>9.021663</td>\n",
       "      <td>18.889168</td>\n",
       "      <td>44.288201</td>\n",
       "      <td>4.017602</td>\n",
       "      <td>0.021663</td>\n",
       "      <td>12.847292</td>\n",
       "      <td>0.164410</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017408</td>\n",
       "      <td>0.038685</td>\n",
       "      <td>0.003868</td>\n",
       "      <td>0.061896</td>\n",
       "      <td>0.032882</td>\n",
       "      <td>0.104449</td>\n",
       "      <td>0.003868</td>\n",
       "      <td>0.001934</td>\n",
       "      <td>0.029014</td>\n",
       "      <td>0.332689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.520111</td>\n",
       "      <td>64.046482</td>\n",
       "      <td>248.066192</td>\n",
       "      <td>4.559477</td>\n",
       "      <td>5.806625</td>\n",
       "      <td>16.317469</td>\n",
       "      <td>1.791653</td>\n",
       "      <td>0.295959</td>\n",
       "      <td>63.655818</td>\n",
       "      <td>0.371006</td>\n",
       "      <td>...</td>\n",
       "      <td>0.130913</td>\n",
       "      <td>0.193029</td>\n",
       "      <td>0.062137</td>\n",
       "      <td>0.241199</td>\n",
       "      <td>0.178500</td>\n",
       "      <td>0.306138</td>\n",
       "      <td>0.062137</td>\n",
       "      <td>0.043980</td>\n",
       "      <td>0.168007</td>\n",
       "      <td>0.471632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>18.700000</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>7.900000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>90.200000</td>\n",
       "      <td>68.600000</td>\n",
       "      <td>437.700000</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>15.500000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>2.700000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>91.600000</td>\n",
       "      <td>108.300000</td>\n",
       "      <td>664.200000</td>\n",
       "      <td>8.400000</td>\n",
       "      <td>19.300000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>92.900000</td>\n",
       "      <td>142.400000</td>\n",
       "      <td>713.900000</td>\n",
       "      <td>10.800000</td>\n",
       "      <td>22.800000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>4.900000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.570000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>96.200000</td>\n",
       "      <td>291.300000</td>\n",
       "      <td>860.600000</td>\n",
       "      <td>56.100000</td>\n",
       "      <td>33.300000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>9.400000</td>\n",
       "      <td>6.400000</td>\n",
       "      <td>1090.840000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             FFMC         DMC          DC         ISI        temp          RH  \\\n",
       "count  517.000000  517.000000  517.000000  517.000000  517.000000  517.000000   \n",
       "mean    90.644681  110.872340  547.940039    9.021663   18.889168   44.288201   \n",
       "std      5.520111   64.046482  248.066192    4.559477    5.806625   16.317469   \n",
       "min     18.700000    1.100000    7.900000    0.000000    2.200000   15.000000   \n",
       "25%     90.200000   68.600000  437.700000    6.500000   15.500000   33.000000   \n",
       "50%     91.600000  108.300000  664.200000    8.400000   19.300000   42.000000   \n",
       "75%     92.900000  142.400000  713.900000   10.800000   22.800000   53.000000   \n",
       "max     96.200000  291.300000  860.600000   56.100000   33.300000  100.000000   \n",
       "\n",
       "             wind        rain         area      dayfri  ...    monthdec  \\\n",
       "count  517.000000  517.000000   517.000000  517.000000  ...  517.000000   \n",
       "mean     4.017602    0.021663    12.847292    0.164410  ...    0.017408   \n",
       "std      1.791653    0.295959    63.655818    0.371006  ...    0.130913   \n",
       "min      0.400000    0.000000     0.000000    0.000000  ...    0.000000   \n",
       "25%      2.700000    0.000000     0.000000    0.000000  ...    0.000000   \n",
       "50%      4.000000    0.000000     0.520000    0.000000  ...    0.000000   \n",
       "75%      4.900000    0.000000     6.570000    0.000000  ...    0.000000   \n",
       "max      9.400000    6.400000  1090.840000    1.000000  ...    1.000000   \n",
       "\n",
       "         monthfeb    monthjan    monthjul    monthjun    monthmar    monthmay  \\\n",
       "count  517.000000  517.000000  517.000000  517.000000  517.000000  517.000000   \n",
       "mean     0.038685    0.003868    0.061896    0.032882    0.104449    0.003868   \n",
       "std      0.193029    0.062137    0.241199    0.178500    0.306138    0.062137   \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "50%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "75%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "max      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "         monthnov    monthoct    monthsep  \n",
       "count  517.000000  517.000000  517.000000  \n",
       "mean     0.001934    0.029014    0.332689  \n",
       "std      0.043980    0.168007    0.471632  \n",
       "min      0.000000    0.000000    0.000000  \n",
       "25%      0.000000    0.000000    0.000000  \n",
       "50%      0.000000    0.000000    0.000000  \n",
       "75%      0.000000    0.000000    1.000000  \n",
       "max      1.000000    1.000000    1.000000  \n",
       "\n",
       "[8 rows x 28 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fire.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7433b9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Since there are many columns we use PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c821693d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-8.05959472e-01, -1.32332557e+00, -1.83047676e+00, ...,\n",
       "        -4.40225453e-02, -1.72859706e-01, -7.06081245e-01],\n",
       "       [-8.10203395e-03, -1.17954077e+00,  4.88890915e-01, ...,\n",
       "        -4.40225453e-02,  5.78503817e+00, -7.06081245e-01],\n",
       "       [-8.10203395e-03, -1.04982188e+00,  5.60715454e-01, ...,\n",
       "        -4.40225453e-02,  5.78503817e+00, -7.06081245e-01],\n",
       "       ...,\n",
       "       [-1.64008316e+00, -8.46647711e-01,  4.74768113e-01, ...,\n",
       "        -4.40225453e-02, -1.72859706e-01, -7.06081245e-01],\n",
       "       [ 6.80956663e-01,  5.49002541e-01,  2.69382214e-01, ...,\n",
       "        -4.40225453e-02, -1.72859706e-01, -7.06081245e-01],\n",
       "       [-2.02087875e+00, -1.68591332e+00, -1.78044169e+00, ...,\n",
       "         2.27156334e+01, -1.72859706e-01, -7.06081245e-01]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1=fire.iloc[:,2:30]\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "sc.fit(df1)\n",
    "\n",
    "df_norm = sc.transform(df1)\n",
    "df_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0600e4ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.76670947e+00, -1.32025451e+00, -8.43971398e-01, ...,\n",
       "        -6.53345819e-02,  4.98037274e-16, -2.73530281e-16],\n",
       "       [ 3.90786263e-01,  8.31061522e-01, -1.10136513e+00, ...,\n",
       "         3.42618601e-02, -9.55928328e-15,  1.15055466e-15],\n",
       "       [ 6.90415596e-01,  1.17774562e+00, -1.22199841e+00, ...,\n",
       "         2.63235187e-02,  2.58690766e-15, -5.66797432e-17],\n",
       "       ...,\n",
       "       [ 9.21634000e-01, -2.64543072e-01,  2.71921606e+00, ...,\n",
       "        -2.97865814e-01, -1.84247930e-16,  2.36645381e-16],\n",
       "       [-1.62054896e+00, -9.78838231e-01,  3.31987355e-01, ...,\n",
       "         3.91949863e-02, -2.30354869e-16,  2.72058887e-16],\n",
       "       [ 4.07590654e+00, -3.67440726e-01, -2.47151775e-01, ...,\n",
       "        -2.50420726e-02,  5.70142521e-17,  8.50237385e-17]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA  \n",
    "\n",
    "pca = PCA(n_components = 28)      #since there are many columns we use pca\n",
    "pca_values =pca.fit_transform(df_norm)\n",
    "pca_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c2a4e8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.35522746e-01, 6.85788793e-02, 6.23572652e-02, 5.32713255e-02,\n",
       "       4.75942360e-02, 4.68009902e-02, 4.37490015e-02, 4.28025164e-02,\n",
       "       4.08875728e-02, 4.01633268e-02, 3.92926854e-02, 3.83232321e-02,\n",
       "       3.64221503e-02, 3.63217289e-02, 3.57856782e-02, 3.50087806e-02,\n",
       "       3.35447704e-02, 3.24777366e-02, 3.04490902e-02, 3.00246758e-02,\n",
       "       2.37167400e-02, 2.08329788e-02, 1.18357869e-02, 8.88449559e-03,\n",
       "       4.55347471e-03, 7.98135931e-04, 2.67271490e-32, 1.95971390e-33])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var = pca.explained_variance_ratio_\n",
    "var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19186860",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13.55, 20.41, 26.65, 31.98, 36.74, 41.42, 45.79, 50.07, 54.16,\n",
       "       58.18, 62.11, 65.94, 69.58, 73.21, 76.79, 80.29, 83.64, 86.89,\n",
       "       89.93, 92.93, 95.3 , 97.38, 98.56, 99.45, 99.91, 99.99, 99.99,\n",
       "       99.99])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var1 = np.cumsum(np.round(var, decimals=4)*100)\n",
    "var1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee4c916c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(var1, color='blue');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7269ca48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pc1</th>\n",
       "      <th>pc2</th>\n",
       "      <th>pc3</th>\n",
       "      <th>pc4</th>\n",
       "      <th>pc5</th>\n",
       "      <th>pc6</th>\n",
       "      <th>pc7</th>\n",
       "      <th>pc8</th>\n",
       "      <th>pc9</th>\n",
       "      <th>pc10</th>\n",
       "      <th>...</th>\n",
       "      <th>pc16</th>\n",
       "      <th>pc17</th>\n",
       "      <th>pc18</th>\n",
       "      <th>pc19</th>\n",
       "      <th>pc20</th>\n",
       "      <th>pc21</th>\n",
       "      <th>pc22</th>\n",
       "      <th>pc23</th>\n",
       "      <th>pc24</th>\n",
       "      <th>size_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.766709</td>\n",
       "      <td>-1.320255</td>\n",
       "      <td>-0.843971</td>\n",
       "      <td>-1.994738</td>\n",
       "      <td>-1.453359</td>\n",
       "      <td>0.693985</td>\n",
       "      <td>0.308104</td>\n",
       "      <td>-0.019764</td>\n",
       "      <td>0.010161</td>\n",
       "      <td>-0.437314</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.197543</td>\n",
       "      <td>-0.021839</td>\n",
       "      <td>0.688958</td>\n",
       "      <td>0.563603</td>\n",
       "      <td>-0.439596</td>\n",
       "      <td>-0.926619</td>\n",
       "      <td>-0.405425</td>\n",
       "      <td>-0.118719</td>\n",
       "      <td>-0.017933</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.390786</td>\n",
       "      <td>0.831062</td>\n",
       "      <td>-1.101365</td>\n",
       "      <td>1.400671</td>\n",
       "      <td>2.869388</td>\n",
       "      <td>0.965898</td>\n",
       "      <td>-2.795574</td>\n",
       "      <td>0.041095</td>\n",
       "      <td>-0.548879</td>\n",
       "      <td>0.104500</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.503167</td>\n",
       "      <td>0.499649</td>\n",
       "      <td>0.563706</td>\n",
       "      <td>-0.703319</td>\n",
       "      <td>-1.535718</td>\n",
       "      <td>-0.892995</td>\n",
       "      <td>0.836590</td>\n",
       "      <td>0.204975</td>\n",
       "      <td>0.290771</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.690416</td>\n",
       "      <td>1.177746</td>\n",
       "      <td>-1.221998</td>\n",
       "      <td>2.442038</td>\n",
       "      <td>1.090630</td>\n",
       "      <td>0.390801</td>\n",
       "      <td>-1.586675</td>\n",
       "      <td>-2.159336</td>\n",
       "      <td>-0.090580</td>\n",
       "      <td>0.260888</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.545144</td>\n",
       "      <td>-0.658411</td>\n",
       "      <td>-0.423618</td>\n",
       "      <td>0.860550</td>\n",
       "      <td>-1.195230</td>\n",
       "      <td>-0.297870</td>\n",
       "      <td>0.743648</td>\n",
       "      <td>0.081757</td>\n",
       "      <td>0.345915</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.359951</td>\n",
       "      <td>-1.161443</td>\n",
       "      <td>0.385728</td>\n",
       "      <td>-2.118328</td>\n",
       "      <td>-1.949601</td>\n",
       "      <td>1.027664</td>\n",
       "      <td>-0.179422</td>\n",
       "      <td>-0.250227</td>\n",
       "      <td>-0.620329</td>\n",
       "      <td>-1.343189</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.040887</td>\n",
       "      <td>0.017843</td>\n",
       "      <td>0.332572</td>\n",
       "      <td>1.164745</td>\n",
       "      <td>-1.632741</td>\n",
       "      <td>-0.817618</td>\n",
       "      <td>1.523710</td>\n",
       "      <td>-0.342302</td>\n",
       "      <td>-0.378420</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.974329</td>\n",
       "      <td>-0.842626</td>\n",
       "      <td>1.327788</td>\n",
       "      <td>0.038086</td>\n",
       "      <td>-1.124763</td>\n",
       "      <td>-0.574676</td>\n",
       "      <td>-0.777155</td>\n",
       "      <td>0.303635</td>\n",
       "      <td>0.861126</td>\n",
       "      <td>-2.024719</td>\n",
       "      <td>...</td>\n",
       "      <td>0.844431</td>\n",
       "      <td>1.014944</td>\n",
       "      <td>-0.618231</td>\n",
       "      <td>0.822853</td>\n",
       "      <td>-1.794109</td>\n",
       "      <td>-0.723371</td>\n",
       "      <td>2.020419</td>\n",
       "      <td>-0.545591</td>\n",
       "      <td>0.161735</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>-0.087560</td>\n",
       "      <td>0.153964</td>\n",
       "      <td>1.241810</td>\n",
       "      <td>1.536581</td>\n",
       "      <td>0.372425</td>\n",
       "      <td>-1.133422</td>\n",
       "      <td>-0.362287</td>\n",
       "      <td>0.766946</td>\n",
       "      <td>0.818745</td>\n",
       "      <td>-0.289632</td>\n",
       "      <td>...</td>\n",
       "      <td>0.300522</td>\n",
       "      <td>0.513876</td>\n",
       "      <td>0.539642</td>\n",
       "      <td>-0.052958</td>\n",
       "      <td>1.898628</td>\n",
       "      <td>-1.441786</td>\n",
       "      <td>-0.821192</td>\n",
       "      <td>-1.205707</td>\n",
       "      <td>-0.698666</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>0.794366</td>\n",
       "      <td>-0.083966</td>\n",
       "      <td>2.670485</td>\n",
       "      <td>0.284995</td>\n",
       "      <td>0.223323</td>\n",
       "      <td>-0.904232</td>\n",
       "      <td>-0.014849</td>\n",
       "      <td>0.107226</td>\n",
       "      <td>1.340049</td>\n",
       "      <td>-0.147246</td>\n",
       "      <td>...</td>\n",
       "      <td>0.342367</td>\n",
       "      <td>0.485571</td>\n",
       "      <td>0.580150</td>\n",
       "      <td>0.384984</td>\n",
       "      <td>0.086251</td>\n",
       "      <td>-0.970693</td>\n",
       "      <td>-1.353365</td>\n",
       "      <td>-1.254890</td>\n",
       "      <td>-1.212175</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>0.921634</td>\n",
       "      <td>-0.264543</td>\n",
       "      <td>2.719216</td>\n",
       "      <td>-0.019643</td>\n",
       "      <td>0.242195</td>\n",
       "      <td>-0.966939</td>\n",
       "      <td>-0.118080</td>\n",
       "      <td>0.123010</td>\n",
       "      <td>1.290364</td>\n",
       "      <td>-0.177553</td>\n",
       "      <td>...</td>\n",
       "      <td>0.332816</td>\n",
       "      <td>0.344047</td>\n",
       "      <td>0.122409</td>\n",
       "      <td>0.313948</td>\n",
       "      <td>0.211157</td>\n",
       "      <td>-0.777731</td>\n",
       "      <td>-1.736711</td>\n",
       "      <td>-1.154127</td>\n",
       "      <td>-1.230040</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>-1.620549</td>\n",
       "      <td>-0.978838</td>\n",
       "      <td>0.331987</td>\n",
       "      <td>1.256638</td>\n",
       "      <td>-0.408164</td>\n",
       "      <td>0.735698</td>\n",
       "      <td>0.815510</td>\n",
       "      <td>-1.398344</td>\n",
       "      <td>0.076379</td>\n",
       "      <td>-0.005814</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011739</td>\n",
       "      <td>-1.035533</td>\n",
       "      <td>-0.774382</td>\n",
       "      <td>-0.216315</td>\n",
       "      <td>0.515791</td>\n",
       "      <td>0.080575</td>\n",
       "      <td>-0.055548</td>\n",
       "      <td>-0.067502</td>\n",
       "      <td>-0.311027</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>4.075907</td>\n",
       "      <td>-0.367441</td>\n",
       "      <td>-0.247152</td>\n",
       "      <td>0.979966</td>\n",
       "      <td>6.792273</td>\n",
       "      <td>5.943666</td>\n",
       "      <td>-1.639583</td>\n",
       "      <td>8.121827</td>\n",
       "      <td>-0.627980</td>\n",
       "      <td>4.953722</td>\n",
       "      <td>...</td>\n",
       "      <td>10.467443</td>\n",
       "      <td>-7.333036</td>\n",
       "      <td>0.377340</td>\n",
       "      <td>8.870354</td>\n",
       "      <td>-1.074288</td>\n",
       "      <td>2.382433</td>\n",
       "      <td>1.042850</td>\n",
       "      <td>0.296436</td>\n",
       "      <td>0.125099</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>517 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          pc1       pc2       pc3       pc4       pc5       pc6       pc7  \\\n",
       "0    3.766709 -1.320255 -0.843971 -1.994738 -1.453359  0.693985  0.308104   \n",
       "1    0.390786  0.831062 -1.101365  1.400671  2.869388  0.965898 -2.795574   \n",
       "2    0.690416  1.177746 -1.221998  2.442038  1.090630  0.390801 -1.586675   \n",
       "3    3.359951 -1.161443  0.385728 -2.118328 -1.949601  1.027664 -0.179422   \n",
       "4    2.974329 -0.842626  1.327788  0.038086 -1.124763 -0.574676 -0.777155   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "512 -0.087560  0.153964  1.241810  1.536581  0.372425 -1.133422 -0.362287   \n",
       "513  0.794366 -0.083966  2.670485  0.284995  0.223323 -0.904232 -0.014849   \n",
       "514  0.921634 -0.264543  2.719216 -0.019643  0.242195 -0.966939 -0.118080   \n",
       "515 -1.620549 -0.978838  0.331987  1.256638 -0.408164  0.735698  0.815510   \n",
       "516  4.075907 -0.367441 -0.247152  0.979966  6.792273  5.943666 -1.639583   \n",
       "\n",
       "          pc8       pc9      pc10  ...       pc16      pc17      pc18  \\\n",
       "0   -0.019764  0.010161 -0.437314  ...  -0.197543 -0.021839  0.688958   \n",
       "1    0.041095 -0.548879  0.104500  ...  -2.503167  0.499649  0.563706   \n",
       "2   -2.159336 -0.090580  0.260888  ...  -2.545144 -0.658411 -0.423618   \n",
       "3   -0.250227 -0.620329 -1.343189  ...  -0.040887  0.017843  0.332572   \n",
       "4    0.303635  0.861126 -2.024719  ...   0.844431  1.014944 -0.618231   \n",
       "..        ...       ...       ...  ...        ...       ...       ...   \n",
       "512  0.766946  0.818745 -0.289632  ...   0.300522  0.513876  0.539642   \n",
       "513  0.107226  1.340049 -0.147246  ...   0.342367  0.485571  0.580150   \n",
       "514  0.123010  1.290364 -0.177553  ...   0.332816  0.344047  0.122409   \n",
       "515 -1.398344  0.076379 -0.005814  ...  -0.011739 -1.035533 -0.774382   \n",
       "516  8.121827 -0.627980  4.953722  ...  10.467443 -7.333036  0.377340   \n",
       "\n",
       "         pc19      pc20      pc21      pc22      pc23      pc24  size_category  \n",
       "0    0.563603 -0.439596 -0.926619 -0.405425 -0.118719 -0.017933              0  \n",
       "1   -0.703319 -1.535718 -0.892995  0.836590  0.204975  0.290771              0  \n",
       "2    0.860550 -1.195230 -0.297870  0.743648  0.081757  0.345915              0  \n",
       "3    1.164745 -1.632741 -0.817618  1.523710 -0.342302 -0.378420              0  \n",
       "4    0.822853 -1.794109 -0.723371  2.020419 -0.545591  0.161735              0  \n",
       "..        ...       ...       ...       ...       ...       ...            ...  \n",
       "512 -0.052958  1.898628 -1.441786 -0.821192 -1.205707 -0.698666              1  \n",
       "513  0.384984  0.086251 -0.970693 -1.353365 -1.254890 -1.212175              1  \n",
       "514  0.313948  0.211157 -0.777731 -1.736711 -1.154127 -1.230040              1  \n",
       "515 -0.216315  0.515791  0.080575 -0.055548 -0.067502 -0.311027              0  \n",
       "516  8.870354 -1.074288  2.382433  1.042850  0.296436  0.125099              0  \n",
       "\n",
       "[517 rows x 25 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalDf = pd.concat([pd.DataFrame(pca_values[:,0:24], columns = ['pc1','pc2','pc3','pc4','pc5','pc6','pc7',\n",
    "                                                             'pc8','pc9','pc10','pc11','pc12','pc13','pc14',\n",
    "                                                             'pc15','pc16','pc17','pc18','pc19','pc20','pc21',\n",
    "                                                             'pc22','pc23','pc24']),\n",
    "                     fire[['size_category']]], axis =1)\n",
    "\n",
    "finalDf.size_category.replace(('large', 'small'),(1,0), inplace=True)\n",
    "finalDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4eab5a9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array = finalDf.values\n",
    "X = array[:,0:24]\n",
    "Y = array[:,24]\n",
    "\n",
    "X.reshape(-1,1)\n",
    "Y.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98531e9",
   "metadata": {},
   "source": [
    "# Iteration-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7dd99896",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=24, activation = 'relu'))\n",
    "model.add(Dense(8, activation = 'relu'))\n",
    "model.add(Dense(1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5428f73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48ce1e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "37/37 [==============================] - 1s 9ms/step - loss: 0.6049 - accuracy: 0.7424 - val_loss: 0.7345 - val_accuracy: 0.6859\n",
      "Epoch 2/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.5790 - accuracy: 0.7535 - val_loss: 0.7328 - val_accuracy: 0.6731\n",
      "Epoch 3/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.5612 - accuracy: 0.7535 - val_loss: 0.7352 - val_accuracy: 0.6731\n",
      "Epoch 4/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.5480 - accuracy: 0.7562 - val_loss: 0.7365 - val_accuracy: 0.6731\n",
      "Epoch 5/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.5377 - accuracy: 0.7562 - val_loss: 0.7375 - val_accuracy: 0.6731\n",
      "Epoch 6/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.5290 - accuracy: 0.7562 - val_loss: 0.7376 - val_accuracy: 0.6731\n",
      "Epoch 7/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.5231 - accuracy: 0.7562 - val_loss: 0.7396 - val_accuracy: 0.6731\n",
      "Epoch 8/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.5172 - accuracy: 0.7562 - val_loss: 0.7365 - val_accuracy: 0.6731\n",
      "Epoch 9/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.5125 - accuracy: 0.7590 - val_loss: 0.7346 - val_accuracy: 0.6731\n",
      "Epoch 10/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.5056 - accuracy: 0.7618 - val_loss: 0.7399 - val_accuracy: 0.6731\n",
      "Epoch 11/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.5001 - accuracy: 0.7618 - val_loss: 0.7445 - val_accuracy: 0.6795\n",
      "Epoch 12/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4948 - accuracy: 0.7645 - val_loss: 0.7441 - val_accuracy: 0.6859\n",
      "Epoch 13/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4898 - accuracy: 0.7645 - val_loss: 0.7516 - val_accuracy: 0.6859\n",
      "Epoch 14/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4853 - accuracy: 0.7645 - val_loss: 0.7518 - val_accuracy: 0.6859\n",
      "Epoch 15/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4815 - accuracy: 0.7645 - val_loss: 0.7551 - val_accuracy: 0.6859\n",
      "Epoch 16/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4766 - accuracy: 0.7645 - val_loss: 0.7599 - val_accuracy: 0.6859\n",
      "Epoch 17/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4720 - accuracy: 0.7645 - val_loss: 0.7618 - val_accuracy: 0.6859\n",
      "Epoch 18/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4676 - accuracy: 0.7645 - val_loss: 0.7625 - val_accuracy: 0.6859\n",
      "Epoch 19/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4622 - accuracy: 0.7673 - val_loss: 0.7623 - val_accuracy: 0.6859\n",
      "Epoch 20/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4561 - accuracy: 0.7756 - val_loss: 0.7680 - val_accuracy: 0.6923\n",
      "Epoch 21/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4517 - accuracy: 0.7812 - val_loss: 0.7612 - val_accuracy: 0.6859\n",
      "Epoch 22/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4442 - accuracy: 0.7895 - val_loss: 0.7625 - val_accuracy: 0.6859\n",
      "Epoch 23/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4386 - accuracy: 0.7978 - val_loss: 0.7633 - val_accuracy: 0.6859\n",
      "Epoch 24/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4328 - accuracy: 0.7978 - val_loss: 0.7776 - val_accuracy: 0.7051\n",
      "Epoch 25/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4259 - accuracy: 0.8061 - val_loss: 0.7749 - val_accuracy: 0.7051\n",
      "Epoch 26/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4203 - accuracy: 0.8061 - val_loss: 0.7771 - val_accuracy: 0.7051\n",
      "Epoch 27/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4133 - accuracy: 0.8089 - val_loss: 0.7810 - val_accuracy: 0.6987\n",
      "Epoch 28/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4056 - accuracy: 0.8172 - val_loss: 0.7882 - val_accuracy: 0.6987\n",
      "Epoch 29/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3961 - accuracy: 0.8227 - val_loss: 0.7832 - val_accuracy: 0.6859\n",
      "Epoch 30/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3910 - accuracy: 0.8283 - val_loss: 0.7962 - val_accuracy: 0.6859\n",
      "Epoch 31/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3834 - accuracy: 0.8310 - val_loss: 0.7910 - val_accuracy: 0.6923\n",
      "Epoch 32/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3769 - accuracy: 0.8421 - val_loss: 0.7905 - val_accuracy: 0.6859\n",
      "Epoch 33/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3717 - accuracy: 0.8366 - val_loss: 0.7960 - val_accuracy: 0.6923\n",
      "Epoch 34/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3648 - accuracy: 0.8476 - val_loss: 0.7978 - val_accuracy: 0.6923\n",
      "Epoch 35/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3565 - accuracy: 0.8532 - val_loss: 0.8011 - val_accuracy: 0.6923\n",
      "Epoch 36/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3495 - accuracy: 0.8587 - val_loss: 0.8074 - val_accuracy: 0.6923\n",
      "Epoch 37/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3459 - accuracy: 0.8587 - val_loss: 0.8024 - val_accuracy: 0.7115\n",
      "Epoch 38/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3409 - accuracy: 0.8670 - val_loss: 0.8034 - val_accuracy: 0.7115\n",
      "Epoch 39/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3338 - accuracy: 0.8643 - val_loss: 0.8124 - val_accuracy: 0.7179\n",
      "Epoch 40/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3278 - accuracy: 0.8615 - val_loss: 0.8156 - val_accuracy: 0.6987\n",
      "Epoch 41/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3226 - accuracy: 0.8615 - val_loss: 0.8197 - val_accuracy: 0.7179\n",
      "Epoch 42/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3173 - accuracy: 0.8670 - val_loss: 0.8242 - val_accuracy: 0.7244\n",
      "Epoch 43/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3119 - accuracy: 0.8670 - val_loss: 0.8307 - val_accuracy: 0.7244\n",
      "Epoch 44/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3088 - accuracy: 0.8753 - val_loss: 0.8231 - val_accuracy: 0.7179\n",
      "Epoch 45/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3038 - accuracy: 0.8726 - val_loss: 0.8270 - val_accuracy: 0.7244\n",
      "Epoch 46/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2976 - accuracy: 0.8781 - val_loss: 0.8313 - val_accuracy: 0.7308\n",
      "Epoch 47/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2907 - accuracy: 0.8781 - val_loss: 0.8377 - val_accuracy: 0.7308\n",
      "Epoch 48/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2871 - accuracy: 0.8837 - val_loss: 0.8429 - val_accuracy: 0.7115\n",
      "Epoch 49/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2812 - accuracy: 0.8809 - val_loss: 0.8473 - val_accuracy: 0.7308\n",
      "Epoch 50/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2756 - accuracy: 0.8920 - val_loss: 0.8512 - val_accuracy: 0.7115\n",
      "Epoch 51/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2698 - accuracy: 0.8975 - val_loss: 0.8535 - val_accuracy: 0.7436\n",
      "Epoch 52/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2675 - accuracy: 0.8975 - val_loss: 0.8583 - val_accuracy: 0.7436\n",
      "Epoch 53/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2610 - accuracy: 0.8975 - val_loss: 0.8615 - val_accuracy: 0.7179\n",
      "Epoch 54/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2582 - accuracy: 0.9003 - val_loss: 0.8584 - val_accuracy: 0.7179\n",
      "Epoch 55/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2541 - accuracy: 0.9003 - val_loss: 0.8637 - val_accuracy: 0.7051\n",
      "Epoch 56/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2504 - accuracy: 0.9030 - val_loss: 0.8590 - val_accuracy: 0.7051\n",
      "Epoch 57/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2451 - accuracy: 0.9030 - val_loss: 0.8625 - val_accuracy: 0.7308\n",
      "Epoch 58/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2418 - accuracy: 0.9058 - val_loss: 0.8648 - val_accuracy: 0.7244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2404 - accuracy: 0.9058 - val_loss: 0.8722 - val_accuracy: 0.7308\n",
      "Epoch 60/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2345 - accuracy: 0.9058 - val_loss: 0.8723 - val_accuracy: 0.7308\n",
      "Epoch 61/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2304 - accuracy: 0.9114 - val_loss: 0.8787 - val_accuracy: 0.7308\n",
      "Epoch 62/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2276 - accuracy: 0.9141 - val_loss: 0.8835 - val_accuracy: 0.7308\n",
      "Epoch 63/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2235 - accuracy: 0.9169 - val_loss: 0.8852 - val_accuracy: 0.7308\n",
      "Epoch 64/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2234 - accuracy: 0.9114 - val_loss: 0.8881 - val_accuracy: 0.7308\n",
      "Epoch 65/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2196 - accuracy: 0.9141 - val_loss: 0.8932 - val_accuracy: 0.7115\n",
      "Epoch 66/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2185 - accuracy: 0.9252 - val_loss: 0.8929 - val_accuracy: 0.7308\n",
      "Epoch 67/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2116 - accuracy: 0.9224 - val_loss: 0.8969 - val_accuracy: 0.7244\n",
      "Epoch 68/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2081 - accuracy: 0.9252 - val_loss: 0.9008 - val_accuracy: 0.7372\n",
      "Epoch 69/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2055 - accuracy: 0.9252 - val_loss: 0.9002 - val_accuracy: 0.7500\n",
      "Epoch 70/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2027 - accuracy: 0.9307 - val_loss: 0.8973 - val_accuracy: 0.7500\n",
      "Epoch 71/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2011 - accuracy: 0.9280 - val_loss: 0.9049 - val_accuracy: 0.7564\n",
      "Epoch 72/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1975 - accuracy: 0.9307 - val_loss: 0.9036 - val_accuracy: 0.7692\n",
      "Epoch 73/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1948 - accuracy: 0.9280 - val_loss: 0.9074 - val_accuracy: 0.7756\n",
      "Epoch 74/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1922 - accuracy: 0.9307 - val_loss: 0.9061 - val_accuracy: 0.7628\n",
      "Epoch 75/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.1906 - accuracy: 0.9280 - val_loss: 0.9145 - val_accuracy: 0.7564\n",
      "Epoch 76/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.1877 - accuracy: 0.9307 - val_loss: 0.9140 - val_accuracy: 0.7564\n",
      "Epoch 77/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.1838 - accuracy: 0.9307 - val_loss: 0.9185 - val_accuracy: 0.7756\n",
      "Epoch 78/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.1809 - accuracy: 0.9307 - val_loss: 0.9253 - val_accuracy: 0.7756\n",
      "Epoch 79/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1779 - accuracy: 0.9335 - val_loss: 0.9261 - val_accuracy: 0.7821\n",
      "Epoch 80/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1779 - accuracy: 0.9335 - val_loss: 0.9361 - val_accuracy: 0.7821\n",
      "Epoch 81/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1735 - accuracy: 0.9335 - val_loss: 0.9334 - val_accuracy: 0.7821\n",
      "Epoch 82/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1727 - accuracy: 0.9391 - val_loss: 0.9347 - val_accuracy: 0.7821\n",
      "Epoch 83/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1698 - accuracy: 0.9391 - val_loss: 0.9384 - val_accuracy: 0.7821\n",
      "Epoch 84/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1666 - accuracy: 0.9391 - val_loss: 0.9408 - val_accuracy: 0.7821\n",
      "Epoch 85/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1659 - accuracy: 0.9391 - val_loss: 0.9426 - val_accuracy: 0.7821\n",
      "Epoch 86/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1680 - accuracy: 0.9446 - val_loss: 0.9356 - val_accuracy: 0.7821\n",
      "Epoch 87/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1632 - accuracy: 0.9501 - val_loss: 0.9456 - val_accuracy: 0.7885\n",
      "Epoch 88/150\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.1590 - accuracy: 0.9418 - val_loss: 0.9602 - val_accuracy: 0.7821\n",
      "Epoch 89/150\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.1575 - accuracy: 0.9391 - val_loss: 0.9576 - val_accuracy: 0.7885\n",
      "Epoch 90/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1544 - accuracy: 0.9474 - val_loss: 0.9688 - val_accuracy: 0.7885\n",
      "Epoch 91/150\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.1525 - accuracy: 0.9391 - val_loss: 0.9695 - val_accuracy: 0.7885\n",
      "Epoch 92/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1518 - accuracy: 0.9446 - val_loss: 0.9858 - val_accuracy: 0.7885\n",
      "Epoch 93/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1608 - accuracy: 0.9391 - val_loss: 0.9966 - val_accuracy: 0.7628\n",
      "Epoch 94/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1500 - accuracy: 0.9446 - val_loss: 0.9934 - val_accuracy: 0.7949\n",
      "Epoch 95/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1434 - accuracy: 0.9612 - val_loss: 0.9888 - val_accuracy: 0.7949\n",
      "Epoch 96/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1403 - accuracy: 0.9501 - val_loss: 0.9962 - val_accuracy: 0.8013\n",
      "Epoch 97/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1370 - accuracy: 0.9612 - val_loss: 0.9940 - val_accuracy: 0.8077\n",
      "Epoch 98/150\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.1397 - accuracy: 0.9557 - val_loss: 1.0171 - val_accuracy: 0.7628\n",
      "Epoch 99/150\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.1390 - accuracy: 0.9529 - val_loss: 1.0104 - val_accuracy: 0.8077\n",
      "Epoch 100/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1305 - accuracy: 0.9640 - val_loss: 1.0208 - val_accuracy: 0.8077\n",
      "Epoch 101/150\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.1320 - accuracy: 0.9501 - val_loss: 1.0252 - val_accuracy: 0.8013\n",
      "Epoch 102/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1288 - accuracy: 0.9612 - val_loss: 1.0341 - val_accuracy: 0.8013\n",
      "Epoch 103/150\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.1253 - accuracy: 0.9584 - val_loss: 1.0297 - val_accuracy: 0.8205\n",
      "Epoch 104/150\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.1417 - accuracy: 0.9612 - val_loss: 1.0383 - val_accuracy: 0.8141\n",
      "Epoch 105/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1271 - accuracy: 0.9501 - val_loss: 1.0507 - val_accuracy: 0.8141\n",
      "Epoch 106/150\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.1202 - accuracy: 0.9695 - val_loss: 1.0501 - val_accuracy: 0.8141\n",
      "Epoch 107/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1186 - accuracy: 0.9668 - val_loss: 1.0655 - val_accuracy: 0.8205\n",
      "Epoch 108/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1145 - accuracy: 0.9640 - val_loss: 1.0681 - val_accuracy: 0.8141\n",
      "Epoch 109/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1118 - accuracy: 0.9695 - val_loss: 1.0762 - val_accuracy: 0.8205\n",
      "Epoch 110/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1093 - accuracy: 0.9695 - val_loss: 1.0855 - val_accuracy: 0.8269\n",
      "Epoch 111/150\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.1080 - accuracy: 0.9723 - val_loss: 1.0925 - val_accuracy: 0.8205\n",
      "Epoch 112/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1071 - accuracy: 0.9723 - val_loss: 1.1027 - val_accuracy: 0.8141\n",
      "Epoch 113/150\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.1054 - accuracy: 0.9695 - val_loss: 1.1000 - val_accuracy: 0.8205\n",
      "Epoch 114/150\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.1046 - accuracy: 0.9695 - val_loss: 1.1153 - val_accuracy: 0.8333\n",
      "Epoch 115/150\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.1026 - accuracy: 0.9723 - val_loss: 1.1304 - val_accuracy: 0.8205\n",
      "Epoch 116/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 0s 4ms/step - loss: 0.0981 - accuracy: 0.9751 - val_loss: 1.1356 - val_accuracy: 0.8333\n",
      "Epoch 117/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0974 - accuracy: 0.9751 - val_loss: 1.1419 - val_accuracy: 0.8333\n",
      "Epoch 118/150\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.1110 - accuracy: 0.9806 - val_loss: 1.1594 - val_accuracy: 0.8269\n",
      "Epoch 119/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0944 - accuracy: 0.9834 - val_loss: 1.1694 - val_accuracy: 0.8269\n",
      "Epoch 120/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0928 - accuracy: 0.9723 - val_loss: 1.1764 - val_accuracy: 0.8269\n",
      "Epoch 121/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0885 - accuracy: 0.9778 - val_loss: 1.1880 - val_accuracy: 0.8333\n",
      "Epoch 122/150\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.0845 - accuracy: 0.9806 - val_loss: 1.1938 - val_accuracy: 0.8333\n",
      "Epoch 123/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0832 - accuracy: 0.9778 - val_loss: 1.2071 - val_accuracy: 0.8333\n",
      "Epoch 124/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0833 - accuracy: 0.9834 - val_loss: 1.2153 - val_accuracy: 0.8269\n",
      "Epoch 125/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0806 - accuracy: 0.9834 - val_loss: 1.2247 - val_accuracy: 0.8397\n",
      "Epoch 126/150\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.0791 - accuracy: 0.9751 - val_loss: 1.2256 - val_accuracy: 0.8397\n",
      "Epoch 127/150\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.0794 - accuracy: 0.9806 - val_loss: 1.2441 - val_accuracy: 0.8397\n",
      "Epoch 128/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0740 - accuracy: 0.9806 - val_loss: 1.2499 - val_accuracy: 0.8397\n",
      "Epoch 129/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0734 - accuracy: 0.9751 - val_loss: 1.2638 - val_accuracy: 0.8397\n",
      "Epoch 130/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0727 - accuracy: 0.9834 - val_loss: 1.2648 - val_accuracy: 0.8397\n",
      "Epoch 131/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0778 - accuracy: 0.9972 - val_loss: 1.2756 - val_accuracy: 0.8397\n",
      "Epoch 132/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0750 - accuracy: 0.9806 - val_loss: 1.2926 - val_accuracy: 0.8397\n",
      "Epoch 133/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0955 - accuracy: 0.9806 - val_loss: 1.3399 - val_accuracy: 0.8397\n",
      "Epoch 134/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0955 - accuracy: 0.9751 - val_loss: 1.3514 - val_accuracy: 0.8397\n",
      "Epoch 135/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0835 - accuracy: 0.9806 - val_loss: 1.3381 - val_accuracy: 0.8397\n",
      "Epoch 136/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0750 - accuracy: 0.9834 - val_loss: 1.3350 - val_accuracy: 0.8397\n",
      "Epoch 137/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0673 - accuracy: 0.9861 - val_loss: 1.3371 - val_accuracy: 0.8397\n",
      "Epoch 138/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0625 - accuracy: 0.9972 - val_loss: 1.3386 - val_accuracy: 0.8397\n",
      "Epoch 139/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0625 - accuracy: 0.9861 - val_loss: 1.3408 - val_accuracy: 0.8397\n",
      "Epoch 140/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0613 - accuracy: 0.9861 - val_loss: 1.3549 - val_accuracy: 0.8397\n",
      "Epoch 141/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0588 - accuracy: 0.9917 - val_loss: 1.3613 - val_accuracy: 0.8397\n",
      "Epoch 142/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0588 - accuracy: 0.9917 - val_loss: 1.3643 - val_accuracy: 0.8397\n",
      "Epoch 143/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0576 - accuracy: 0.9861 - val_loss: 1.3718 - val_accuracy: 0.8397\n",
      "Epoch 144/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0563 - accuracy: 0.9917 - val_loss: 1.3821 - val_accuracy: 0.8397\n",
      "Epoch 145/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0568 - accuracy: 0.9945 - val_loss: 1.3901 - val_accuracy: 0.8397\n",
      "Epoch 146/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0646 - accuracy: 0.9861 - val_loss: 1.3754 - val_accuracy: 0.8397\n",
      "Epoch 147/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0580 - accuracy: 0.9861 - val_loss: 1.3918 - val_accuracy: 0.8397\n",
      "Epoch 148/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.0545 - accuracy: 0.9917 - val_loss: 1.4051 - val_accuracy: 0.8397\n",
      "Epoch 149/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0527 - accuracy: 0.9945 - val_loss: 1.4076 - val_accuracy: 0.8397\n",
      "Epoch 150/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.0527 - accuracy: 0.9917 - val_loss: 1.4235 - val_accuracy: 0.8397\n"
     ]
    }
   ],
   "source": [
    "#Fit the model\n",
    "model.fit(X, Y, validation_split=0.3, epochs=150, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe4873de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 2ms/step - loss: 0.4635 - accuracy: 0.9458\n",
      "accuracy: 94.58%\n"
     ]
    }
   ],
   "source": [
    "#evaluate the model\n",
    "scores = model.evaluate(X, Y)\n",
    "print(\"%s: %.2f%%\" %(model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a662e329",
   "metadata": {},
   "source": [
    "# Iteration-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53fb67aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=24, activation = 'sigmoid'))\n",
    "model.add(Dense(8,activation='sigmoid'))\n",
    "model.add(Dense(1,activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e43b8121",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea04b14f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "25/25 [==============================] - 1s 12ms/step - loss: 0.5770 - accuracy: 0.7562 - val_loss: 0.6579 - val_accuracy: 0.6731\n",
      "Epoch 2/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.5676 - accuracy: 0.7562 - val_loss: 0.6517 - val_accuracy: 0.6731\n",
      "Epoch 3/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.5627 - accuracy: 0.7562 - val_loss: 0.6453 - val_accuracy: 0.6731\n",
      "Epoch 4/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5568 - accuracy: 0.7562 - val_loss: 0.6570 - val_accuracy: 0.6731\n",
      "Epoch 5/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5524 - accuracy: 0.7562 - val_loss: 0.6582 - val_accuracy: 0.6731\n",
      "Epoch 6/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5473 - accuracy: 0.7562 - val_loss: 0.6476 - val_accuracy: 0.6731\n",
      "Epoch 7/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5439 - accuracy: 0.7562 - val_loss: 0.6376 - val_accuracy: 0.6731\n",
      "Epoch 8/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5389 - accuracy: 0.7562 - val_loss: 0.6481 - val_accuracy: 0.6731\n",
      "Epoch 9/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5333 - accuracy: 0.7562 - val_loss: 0.6366 - val_accuracy: 0.6731\n",
      "Epoch 10/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5315 - accuracy: 0.7562 - val_loss: 0.6567 - val_accuracy: 0.6731\n",
      "Epoch 11/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5232 - accuracy: 0.7590 - val_loss: 0.6451 - val_accuracy: 0.6731\n",
      "Epoch 12/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.5196 - accuracy: 0.7590 - val_loss: 0.6389 - val_accuracy: 0.6731\n",
      "Epoch 13/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5163 - accuracy: 0.7645 - val_loss: 0.6262 - val_accuracy: 0.6795\n",
      "Epoch 14/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5138 - accuracy: 0.7618 - val_loss: 0.6465 - val_accuracy: 0.6795\n",
      "Epoch 15/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.5087 - accuracy: 0.7673 - val_loss: 0.6419 - val_accuracy: 0.6795\n",
      "Epoch 16/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5043 - accuracy: 0.7673 - val_loss: 0.6553 - val_accuracy: 0.6795\n",
      "Epoch 17/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5030 - accuracy: 0.7784 - val_loss: 0.6393 - val_accuracy: 0.6795\n",
      "Epoch 18/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5058 - accuracy: 0.7729 - val_loss: 0.6396 - val_accuracy: 0.6795\n",
      "Epoch 19/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4961 - accuracy: 0.7784 - val_loss: 0.6412 - val_accuracy: 0.6795\n",
      "Epoch 20/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4897 - accuracy: 0.7812 - val_loss: 0.6289 - val_accuracy: 0.6795\n",
      "Epoch 21/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4838 - accuracy: 0.7812 - val_loss: 0.6510 - val_accuracy: 0.6795\n",
      "Epoch 22/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4848 - accuracy: 0.7756 - val_loss: 0.6453 - val_accuracy: 0.6795\n",
      "Epoch 23/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4777 - accuracy: 0.7812 - val_loss: 0.6578 - val_accuracy: 0.6795\n",
      "Epoch 24/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4727 - accuracy: 0.7839 - val_loss: 0.6478 - val_accuracy: 0.6795\n",
      "Epoch 25/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4683 - accuracy: 0.7895 - val_loss: 0.6503 - val_accuracy: 0.6859\n",
      "Epoch 26/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4656 - accuracy: 0.7895 - val_loss: 0.7454 - val_accuracy: 0.6859\n",
      "Epoch 27/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4623 - accuracy: 0.7922 - val_loss: 0.7288 - val_accuracy: 0.6923\n",
      "Epoch 28/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4567 - accuracy: 0.7922 - val_loss: 0.8050 - val_accuracy: 0.6923\n",
      "Epoch 29/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4536 - accuracy: 0.7922 - val_loss: 0.8017 - val_accuracy: 0.6923\n",
      "Epoch 30/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4534 - accuracy: 0.7950 - val_loss: 0.8029 - val_accuracy: 0.6923\n",
      "Epoch 31/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4486 - accuracy: 0.7922 - val_loss: 0.8921 - val_accuracy: 0.6923\n",
      "Epoch 32/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4396 - accuracy: 0.7950 - val_loss: 0.8032 - val_accuracy: 0.7051\n",
      "Epoch 33/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4384 - accuracy: 0.7978 - val_loss: 0.8702 - val_accuracy: 0.7051\n",
      "Epoch 34/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4324 - accuracy: 0.7978 - val_loss: 0.9507 - val_accuracy: 0.7051\n",
      "Epoch 35/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4289 - accuracy: 0.7950 - val_loss: 0.8726 - val_accuracy: 0.6987\n",
      "Epoch 36/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4246 - accuracy: 0.7978 - val_loss: 0.9461 - val_accuracy: 0.7051\n",
      "Epoch 37/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4207 - accuracy: 0.8006 - val_loss: 0.9424 - val_accuracy: 0.6987\n",
      "Epoch 38/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4168 - accuracy: 0.8006 - val_loss: 0.9385 - val_accuracy: 0.6987\n",
      "Epoch 39/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4171 - accuracy: 0.8033 - val_loss: 0.9267 - val_accuracy: 0.6987\n",
      "Epoch 40/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4091 - accuracy: 0.8006 - val_loss: 0.9462 - val_accuracy: 0.6987\n",
      "Epoch 41/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4063 - accuracy: 0.8061 - val_loss: 0.9322 - val_accuracy: 0.6987\n",
      "Epoch 42/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4021 - accuracy: 0.8006 - val_loss: 0.9987 - val_accuracy: 0.6987\n",
      "Epoch 43/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3988 - accuracy: 0.8061 - val_loss: 0.9943 - val_accuracy: 0.6987\n",
      "Epoch 44/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3966 - accuracy: 0.8061 - val_loss: 0.9852 - val_accuracy: 0.6987\n",
      "Epoch 45/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3900 - accuracy: 0.8061 - val_loss: 0.9829 - val_accuracy: 0.6987\n",
      "Epoch 46/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3870 - accuracy: 0.8061 - val_loss: 0.9768 - val_accuracy: 0.6987\n",
      "Epoch 47/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3838 - accuracy: 0.8061 - val_loss: 0.9760 - val_accuracy: 0.6987\n",
      "Epoch 48/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3797 - accuracy: 0.8144 - val_loss: 0.9725 - val_accuracy: 0.7051\n",
      "Epoch 49/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3772 - accuracy: 0.8144 - val_loss: 0.9697 - val_accuracy: 0.7051\n",
      "Epoch 50/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3760 - accuracy: 0.8061 - val_loss: 0.9693 - val_accuracy: 0.7051\n",
      "Epoch 51/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3706 - accuracy: 0.8116 - val_loss: 0.9640 - val_accuracy: 0.7179\n",
      "Epoch 52/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3682 - accuracy: 0.8144 - val_loss: 0.9601 - val_accuracy: 0.7244\n",
      "Epoch 53/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3649 - accuracy: 0.8227 - val_loss: 0.9572 - val_accuracy: 0.7372\n",
      "Epoch 54/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3634 - accuracy: 0.8144 - val_loss: 0.9579 - val_accuracy: 0.7372\n",
      "Epoch 55/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3578 - accuracy: 0.8172 - val_loss: 0.9509 - val_accuracy: 0.7372\n",
      "Epoch 56/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3567 - accuracy: 0.8172 - val_loss: 0.9485 - val_accuracy: 0.7436\n",
      "Epoch 57/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3528 - accuracy: 0.8227 - val_loss: 0.9456 - val_accuracy: 0.7436\n",
      "Epoch 58/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3492 - accuracy: 0.8199 - val_loss: 0.9429 - val_accuracy: 0.7436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3460 - accuracy: 0.8199 - val_loss: 0.9360 - val_accuracy: 0.7692\n",
      "Epoch 60/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3468 - accuracy: 0.8338 - val_loss: 0.9339 - val_accuracy: 0.7821\n",
      "Epoch 61/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3388 - accuracy: 0.8227 - val_loss: 0.9343 - val_accuracy: 0.7564\n",
      "Epoch 62/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3365 - accuracy: 0.8227 - val_loss: 0.9297 - val_accuracy: 0.7628\n",
      "Epoch 63/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3346 - accuracy: 0.8227 - val_loss: 0.9257 - val_accuracy: 0.7821\n",
      "Epoch 64/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3307 - accuracy: 0.8255 - val_loss: 0.9228 - val_accuracy: 0.7692\n",
      "Epoch 65/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3259 - accuracy: 0.8227 - val_loss: 0.9217 - val_accuracy: 0.7821\n",
      "Epoch 66/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3303 - accuracy: 0.8393 - val_loss: 0.8450 - val_accuracy: 0.7885\n",
      "Epoch 67/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3223 - accuracy: 0.8338 - val_loss: 0.9198 - val_accuracy: 0.7821\n",
      "Epoch 68/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3205 - accuracy: 0.8338 - val_loss: 0.9176 - val_accuracy: 0.7821\n",
      "Epoch 69/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3166 - accuracy: 0.8338 - val_loss: 0.9139 - val_accuracy: 0.7885\n",
      "Epoch 70/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3138 - accuracy: 0.8393 - val_loss: 0.9083 - val_accuracy: 0.7885\n",
      "Epoch 71/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3127 - accuracy: 0.8366 - val_loss: 0.9061 - val_accuracy: 0.7885\n",
      "Epoch 72/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3071 - accuracy: 0.8393 - val_loss: 0.9021 - val_accuracy: 0.7885\n",
      "Epoch 73/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3075 - accuracy: 0.8476 - val_loss: 0.9076 - val_accuracy: 0.8013\n",
      "Epoch 74/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3024 - accuracy: 0.8421 - val_loss: 0.9024 - val_accuracy: 0.7949\n",
      "Epoch 75/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.2991 - accuracy: 0.8421 - val_loss: 0.8992 - val_accuracy: 0.7949\n",
      "Epoch 76/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3088 - accuracy: 0.8615 - val_loss: 0.8977 - val_accuracy: 0.7885\n",
      "Epoch 77/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2949 - accuracy: 0.8476 - val_loss: 0.8891 - val_accuracy: 0.7885\n",
      "Epoch 78/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.2936 - accuracy: 0.8560 - val_loss: 0.8886 - val_accuracy: 0.7949\n",
      "Epoch 79/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.2917 - accuracy: 0.8476 - val_loss: 0.8852 - val_accuracy: 0.7949\n",
      "Epoch 80/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.2865 - accuracy: 0.8504 - val_loss: 0.8836 - val_accuracy: 0.7949\n",
      "Epoch 81/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.2842 - accuracy: 0.8504 - val_loss: 0.8837 - val_accuracy: 0.7949\n",
      "Epoch 82/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.2832 - accuracy: 0.8532 - val_loss: 0.8795 - val_accuracy: 0.7949\n",
      "Epoch 83/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.2804 - accuracy: 0.8532 - val_loss: 0.8780 - val_accuracy: 0.7949\n",
      "Epoch 84/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.2767 - accuracy: 0.8532 - val_loss: 0.8757 - val_accuracy: 0.7949\n",
      "Epoch 85/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.2737 - accuracy: 0.8532 - val_loss: 0.8723 - val_accuracy: 0.8013\n",
      "Epoch 86/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.2700 - accuracy: 0.8560 - val_loss: 0.8738 - val_accuracy: 0.7949\n",
      "Epoch 87/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.2709 - accuracy: 0.8587 - val_loss: 0.8711 - val_accuracy: 0.8205\n",
      "Epoch 88/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.2650 - accuracy: 0.8560 - val_loss: 0.8652 - val_accuracy: 0.8013\n",
      "Epoch 89/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2660 - accuracy: 0.8532 - val_loss: 0.8649 - val_accuracy: 0.8013\n",
      "Epoch 90/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.2597 - accuracy: 0.8587 - val_loss: 0.8633 - val_accuracy: 0.8077\n",
      "Epoch 91/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.2582 - accuracy: 0.8560 - val_loss: 0.8600 - val_accuracy: 0.8077\n",
      "Epoch 92/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.2566 - accuracy: 0.8670 - val_loss: 0.8603 - val_accuracy: 0.8269\n",
      "Epoch 93/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.2542 - accuracy: 0.8587 - val_loss: 0.8557 - val_accuracy: 0.8141\n",
      "Epoch 94/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.2512 - accuracy: 0.8726 - val_loss: 0.8674 - val_accuracy: 0.8269\n",
      "Epoch 95/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.2489 - accuracy: 0.8698 - val_loss: 0.8544 - val_accuracy: 0.8269\n",
      "Epoch 96/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.2462 - accuracy: 0.8726 - val_loss: 0.9251 - val_accuracy: 0.8397\n",
      "Epoch 97/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.2447 - accuracy: 0.8643 - val_loss: 0.9206 - val_accuracy: 0.8333\n",
      "Epoch 98/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.2397 - accuracy: 0.8726 - val_loss: 0.9180 - val_accuracy: 0.8269\n",
      "Epoch 99/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.2382 - accuracy: 0.8726 - val_loss: 0.9145 - val_accuracy: 0.8397\n",
      "Epoch 100/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.2361 - accuracy: 0.8781 - val_loss: 0.9108 - val_accuracy: 0.8333\n"
     ]
    }
   ],
   "source": [
    "#Fit the model\n",
    "model.fit(X, Y, validation_split=0.3, epochs=100, batch_size=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d6127ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 1ms/step - loss: 0.4358 - accuracy: 0.8607\n",
      "accuracy: 86.07%\n"
     ]
    }
   ],
   "source": [
    "#evaluate the model\n",
    "scores = model.evaluate(X, Y)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6927f91c",
   "metadata": {},
   "source": [
    "# Iteration-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5dbf9c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=24, activation='relu'))\n",
    "model.add(Dense(8,activation='relu'))\n",
    "model.add(Dense(1,activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "48cc23d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e153d8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "25/25 [==============================] - 1s 10ms/step - loss: 1.3808 - accuracy: 0.5845 - val_loss: 1.5022 - val_accuracy: 0.4936\n",
      "Epoch 2/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.8818 - accuracy: 0.6981 - val_loss: 0.9649 - val_accuracy: 0.5000\n",
      "Epoch 3/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.7627 - accuracy: 0.7175 - val_loss: 0.7979 - val_accuracy: 0.5192\n",
      "Epoch 4/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.6888 - accuracy: 0.7424 - val_loss: 0.7760 - val_accuracy: 0.5192\n",
      "Epoch 5/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.6763 - accuracy: 0.7452 - val_loss: 0.7576 - val_accuracy: 0.5385\n",
      "Epoch 6/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.6665 - accuracy: 0.7590 - val_loss: 0.7466 - val_accuracy: 0.5833\n",
      "Epoch 7/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.6568 - accuracy: 0.7645 - val_loss: 0.7384 - val_accuracy: 0.5641\n",
      "Epoch 8/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.6496 - accuracy: 0.7729 - val_loss: 0.7273 - val_accuracy: 0.5897\n",
      "Epoch 9/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.6415 - accuracy: 0.7756 - val_loss: 0.7239 - val_accuracy: 0.5769\n",
      "Epoch 10/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.6351 - accuracy: 0.7867 - val_loss: 0.7191 - val_accuracy: 0.6090\n",
      "Epoch 11/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.6286 - accuracy: 0.7839 - val_loss: 0.7880 - val_accuracy: 0.6154\n",
      "Epoch 12/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.6232 - accuracy: 0.7839 - val_loss: 0.7807 - val_accuracy: 0.6218\n",
      "Epoch 13/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.6179 - accuracy: 0.7895 - val_loss: 0.6994 - val_accuracy: 0.6346\n",
      "Epoch 14/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.6114 - accuracy: 0.7895 - val_loss: 0.7188 - val_accuracy: 0.6346\n",
      "Epoch 15/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.6053 - accuracy: 0.7950 - val_loss: 0.7705 - val_accuracy: 0.6346\n",
      "Epoch 16/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.5996 - accuracy: 0.7950 - val_loss: 0.7659 - val_accuracy: 0.6474\n",
      "Epoch 17/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5945 - accuracy: 0.8033 - val_loss: 0.7606 - val_accuracy: 0.6410\n",
      "Epoch 18/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.5933 - accuracy: 0.8033 - val_loss: 0.6884 - val_accuracy: 0.6410\n",
      "Epoch 19/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5918 - accuracy: 0.8061 - val_loss: 0.6796 - val_accuracy: 0.6346\n",
      "Epoch 20/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5855 - accuracy: 0.8116 - val_loss: 0.6781 - val_accuracy: 0.6410\n",
      "Epoch 21/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5792 - accuracy: 0.8144 - val_loss: 0.6760 - val_accuracy: 0.6410\n",
      "Epoch 22/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5746 - accuracy: 0.8144 - val_loss: 0.6735 - val_accuracy: 0.6474\n",
      "Epoch 23/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5704 - accuracy: 0.8144 - val_loss: 0.6798 - val_accuracy: 0.6538\n",
      "Epoch 24/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5662 - accuracy: 0.8172 - val_loss: 0.7542 - val_accuracy: 0.6603\n",
      "Epoch 25/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5637 - accuracy: 0.8227 - val_loss: 0.7643 - val_accuracy: 0.6538\n",
      "Epoch 26/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5590 - accuracy: 0.8172 - val_loss: 0.8307 - val_accuracy: 0.6538\n",
      "Epoch 27/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5555 - accuracy: 0.8199 - val_loss: 0.8294 - val_accuracy: 0.6603\n",
      "Epoch 28/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5533 - accuracy: 0.8199 - val_loss: 0.8271 - val_accuracy: 0.6667\n",
      "Epoch 29/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5496 - accuracy: 0.8283 - val_loss: 0.8232 - val_accuracy: 0.6731\n",
      "Epoch 30/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5451 - accuracy: 0.8310 - val_loss: 0.8216 - val_accuracy: 0.6731\n",
      "Epoch 31/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5416 - accuracy: 0.8283 - val_loss: 0.8269 - val_accuracy: 0.6731\n",
      "Epoch 32/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.5393 - accuracy: 0.8255 - val_loss: 0.8268 - val_accuracy: 0.6731\n",
      "Epoch 33/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.5356 - accuracy: 0.8338 - val_loss: 0.8944 - val_accuracy: 0.6538\n",
      "Epoch 34/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.5341 - accuracy: 0.8310 - val_loss: 0.8973 - val_accuracy: 0.6603\n",
      "Epoch 35/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5296 - accuracy: 0.8338 - val_loss: 0.9108 - val_accuracy: 0.6603\n",
      "Epoch 36/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.5265 - accuracy: 0.8366 - val_loss: 0.9901 - val_accuracy: 0.6474\n",
      "Epoch 37/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.5227 - accuracy: 0.8338 - val_loss: 1.0533 - val_accuracy: 0.6603\n",
      "Epoch 38/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4994 - accuracy: 0.8310 - val_loss: 1.1473 - val_accuracy: 0.6667\n",
      "Epoch 39/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4890 - accuracy: 0.8199 - val_loss: 1.0885 - val_accuracy: 0.6538\n",
      "Epoch 40/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4818 - accuracy: 0.8227 - val_loss: 1.0660 - val_accuracy: 0.6474\n",
      "Epoch 41/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.4752 - accuracy: 0.8283 - val_loss: 1.0652 - val_accuracy: 0.6346\n",
      "Epoch 42/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4732 - accuracy: 0.8227 - val_loss: 1.0636 - val_accuracy: 0.6474\n",
      "Epoch 43/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4701 - accuracy: 0.8283 - val_loss: 1.0599 - val_accuracy: 0.6410\n",
      "Epoch 44/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4677 - accuracy: 0.8338 - val_loss: 1.0552 - val_accuracy: 0.6603\n",
      "Epoch 45/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4628 - accuracy: 0.8338 - val_loss: 1.1265 - val_accuracy: 0.6538\n",
      "Epoch 46/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4617 - accuracy: 0.8366 - val_loss: 1.1239 - val_accuracy: 0.6603\n",
      "Epoch 47/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4609 - accuracy: 0.8310 - val_loss: 1.1277 - val_accuracy: 0.6603\n",
      "Epoch 48/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4547 - accuracy: 0.8366 - val_loss: 1.1184 - val_accuracy: 0.6603\n",
      "Epoch 49/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4523 - accuracy: 0.8366 - val_loss: 1.1148 - val_accuracy: 0.6603\n",
      "Epoch 50/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4506 - accuracy: 0.8310 - val_loss: 1.1139 - val_accuracy: 0.6603\n",
      "Epoch 51/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4491 - accuracy: 0.8338 - val_loss: 1.1090 - val_accuracy: 0.6667\n",
      "Epoch 52/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4460 - accuracy: 0.8310 - val_loss: 1.1093 - val_accuracy: 0.6859\n",
      "Epoch 53/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4429 - accuracy: 0.8421 - val_loss: 1.1170 - val_accuracy: 0.6731\n",
      "Epoch 54/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4397 - accuracy: 0.8338 - val_loss: 1.1135 - val_accuracy: 0.6795\n",
      "Epoch 55/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4355 - accuracy: 0.8393 - val_loss: 1.1259 - val_accuracy: 0.6795\n",
      "Epoch 56/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4332 - accuracy: 0.8504 - val_loss: 1.1270 - val_accuracy: 0.6731\n",
      "Epoch 57/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4299 - accuracy: 0.8393 - val_loss: 1.1468 - val_accuracy: 0.6667\n",
      "Epoch 58/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4261 - accuracy: 0.8421 - val_loss: 1.1549 - val_accuracy: 0.6538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4239 - accuracy: 0.8393 - val_loss: 1.1564 - val_accuracy: 0.6474\n",
      "Epoch 60/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4205 - accuracy: 0.8366 - val_loss: 1.1576 - val_accuracy: 0.6474\n",
      "Epoch 61/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4173 - accuracy: 0.8366 - val_loss: 1.1547 - val_accuracy: 0.6474\n",
      "Epoch 62/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4142 - accuracy: 0.8449 - val_loss: 1.1686 - val_accuracy: 0.6410\n",
      "Epoch 63/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4119 - accuracy: 0.8476 - val_loss: 1.2010 - val_accuracy: 0.6474\n",
      "Epoch 64/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4078 - accuracy: 0.8504 - val_loss: 1.1728 - val_accuracy: 0.6474\n",
      "Epoch 65/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4060 - accuracy: 0.8532 - val_loss: 1.2417 - val_accuracy: 0.6474\n",
      "Epoch 66/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.4015 - accuracy: 0.8560 - val_loss: 1.3508 - val_accuracy: 0.6474\n",
      "Epoch 67/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3987 - accuracy: 0.8560 - val_loss: 1.2400 - val_accuracy: 0.6410\n",
      "Epoch 68/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3961 - accuracy: 0.8587 - val_loss: 1.3900 - val_accuracy: 0.6538\n",
      "Epoch 69/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3948 - accuracy: 0.8587 - val_loss: 1.3791 - val_accuracy: 0.6410\n",
      "Epoch 70/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3906 - accuracy: 0.8587 - val_loss: 1.5397 - val_accuracy: 0.6538\n",
      "Epoch 71/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3898 - accuracy: 0.8615 - val_loss: 1.6199 - val_accuracy: 0.6538\n",
      "Epoch 72/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3873 - accuracy: 0.8560 - val_loss: 1.6187 - val_accuracy: 0.6603\n",
      "Epoch 73/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3606 - accuracy: 0.8560 - val_loss: 1.7033 - val_accuracy: 0.6538\n",
      "Epoch 74/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3576 - accuracy: 0.8643 - val_loss: 1.5995 - val_accuracy: 0.6410\n",
      "Epoch 75/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3492 - accuracy: 0.8615 - val_loss: 1.5807 - val_accuracy: 0.6538\n",
      "Epoch 76/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3450 - accuracy: 0.8560 - val_loss: 1.6649 - val_accuracy: 0.6603\n",
      "Epoch 77/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3386 - accuracy: 0.8698 - val_loss: 1.6848 - val_accuracy: 0.6603\n",
      "Epoch 78/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3350 - accuracy: 0.8698 - val_loss: 1.7684 - val_accuracy: 0.6603\n",
      "Epoch 79/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3312 - accuracy: 0.8643 - val_loss: 1.8337 - val_accuracy: 0.6603\n",
      "Epoch 80/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3268 - accuracy: 0.8643 - val_loss: 1.8531 - val_accuracy: 0.6667\n",
      "Epoch 81/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3244 - accuracy: 0.8670 - val_loss: 1.8590 - val_accuracy: 0.6603\n",
      "Epoch 82/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3213 - accuracy: 0.8726 - val_loss: 1.8884 - val_accuracy: 0.6667\n",
      "Epoch 83/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3195 - accuracy: 0.8809 - val_loss: 1.8972 - val_accuracy: 0.6667\n",
      "Epoch 84/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.3155 - accuracy: 0.8781 - val_loss: 1.8993 - val_accuracy: 0.6667\n",
      "Epoch 85/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3124 - accuracy: 0.8753 - val_loss: 2.0378 - val_accuracy: 0.6731\n",
      "Epoch 86/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3109 - accuracy: 0.8781 - val_loss: 2.0616 - val_accuracy: 0.6667\n",
      "Epoch 87/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3109 - accuracy: 0.8864 - val_loss: 2.0454 - val_accuracy: 0.6731\n",
      "Epoch 88/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3065 - accuracy: 0.8864 - val_loss: 1.8625 - val_accuracy: 0.6603\n",
      "Epoch 89/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3045 - accuracy: 0.8892 - val_loss: 2.0480 - val_accuracy: 0.6667\n",
      "Epoch 90/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3015 - accuracy: 0.8920 - val_loss: 2.0408 - val_accuracy: 0.6603\n",
      "Epoch 91/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.2992 - accuracy: 0.8892 - val_loss: 2.0392 - val_accuracy: 0.6731\n",
      "Epoch 92/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.2980 - accuracy: 0.8920 - val_loss: 2.2049 - val_accuracy: 0.6667\n",
      "Epoch 93/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.2950 - accuracy: 0.8975 - val_loss: 2.3800 - val_accuracy: 0.6667\n",
      "Epoch 94/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3059 - accuracy: 0.8892 - val_loss: 2.8961 - val_accuracy: 0.6667\n",
      "Epoch 95/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.2901 - accuracy: 0.8892 - val_loss: 2.4494 - val_accuracy: 0.6667\n",
      "Epoch 96/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.2825 - accuracy: 0.8947 - val_loss: 2.2675 - val_accuracy: 0.6603\n",
      "Epoch 97/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.2817 - accuracy: 0.9030 - val_loss: 2.2995 - val_accuracy: 0.6667\n",
      "Epoch 98/100\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.2766 - accuracy: 0.9058 - val_loss: 2.2657 - val_accuracy: 0.6667\n",
      "Epoch 99/100\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.2729 - accuracy: 0.9030 - val_loss: 2.2645 - val_accuracy: 0.6603\n",
      "Epoch 100/100\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2705 - accuracy: 0.9003 - val_loss: 2.7169 - val_accuracy: 0.6667\n"
     ]
    }
   ],
   "source": [
    "#Fit the model\n",
    "model.fit(X, Y, validation_split=0.3, epochs=100, batch_size=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "82a0c8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 2ms/step - loss: 1.0041 - accuracy: 0.8395\n",
      "accuracy: 83.95%\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the model\n",
    "scores = model.evaluate(X, Y)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b341fedc",
   "metadata": {},
   "source": [
    "# Iteration-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "73360faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=24, activation='relu'))\n",
    "model.add(Dense(8,activation='relu'))\n",
    "model.add(Dense(1,activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3e53d8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d466eca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "37/37 [==============================] - 1s 8ms/step - loss: 3.4377 - accuracy: 0.7535 - val_loss: 4.0382 - val_accuracy: 0.6603\n",
      "Epoch 2/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.3893 - accuracy: 0.7507 - val_loss: 3.9694 - val_accuracy: 0.6538\n",
      "Epoch 3/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.3855 - accuracy: 0.7535 - val_loss: 4.0197 - val_accuracy: 0.6538\n",
      "Epoch 4/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.3800 - accuracy: 0.7562 - val_loss: 4.0983 - val_accuracy: 0.6603\n",
      "Epoch 5/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.3767 - accuracy: 0.7562 - val_loss: 4.1003 - val_accuracy: 0.6603\n",
      "Epoch 6/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.3742 - accuracy: 0.7562 - val_loss: 4.1074 - val_accuracy: 0.6603\n",
      "Epoch 7/150\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 3.3741 - accuracy: 0.7562 - val_loss: 4.1059 - val_accuracy: 0.6603\n",
      "Epoch 8/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.3706 - accuracy: 0.7562 - val_loss: 4.0530 - val_accuracy: 0.6603\n",
      "Epoch 9/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.3699 - accuracy: 0.7562 - val_loss: 4.0625 - val_accuracy: 0.6603\n",
      "Epoch 10/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.3690 - accuracy: 0.7562 - val_loss: 4.1468 - val_accuracy: 0.6667\n",
      "Epoch 11/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.3679 - accuracy: 0.7562 - val_loss: 4.1164 - val_accuracy: 0.6667\n",
      "Epoch 12/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.3386 - accuracy: 0.7535 - val_loss: 3.9326 - val_accuracy: 0.6603\n",
      "Epoch 13/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.2271 - accuracy: 0.7535 - val_loss: 3.7383 - val_accuracy: 0.6603\n",
      "Epoch 14/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.1124 - accuracy: 0.7507 - val_loss: 3.6428 - val_accuracy: 0.6346\n",
      "Epoch 15/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.0752 - accuracy: 0.7562 - val_loss: 3.5795 - val_accuracy: 0.6410\n",
      "Epoch 16/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.9659 - accuracy: 0.7479 - val_loss: 3.4444 - val_accuracy: 0.6603\n",
      "Epoch 17/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.6958 - accuracy: 0.7341 - val_loss: 2.6757 - val_accuracy: 0.5897\n",
      "Epoch 18/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.1277 - accuracy: 0.7147 - val_loss: 2.4499 - val_accuracy: 0.5962\n",
      "Epoch 19/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 2.0148 - accuracy: 0.7313 - val_loss: 2.2126 - val_accuracy: 0.5833\n",
      "Epoch 20/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.8261 - accuracy: 0.7341 - val_loss: 2.0400 - val_accuracy: 0.5833\n",
      "Epoch 21/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.7667 - accuracy: 0.7285 - val_loss: 2.0350 - val_accuracy: 0.6154\n",
      "Epoch 22/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.7143 - accuracy: 0.7341 - val_loss: 2.0751 - val_accuracy: 0.6282\n",
      "Epoch 23/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.7027 - accuracy: 0.7424 - val_loss: 2.0697 - val_accuracy: 0.6346\n",
      "Epoch 24/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.6953 - accuracy: 0.7452 - val_loss: 2.0668 - val_accuracy: 0.6346\n",
      "Epoch 25/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.6902 - accuracy: 0.7479 - val_loss: 2.0681 - val_accuracy: 0.6346\n",
      "Epoch 26/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.6558 - accuracy: 0.7535 - val_loss: 2.0423 - val_accuracy: 0.6474\n",
      "Epoch 27/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.6410 - accuracy: 0.7590 - val_loss: 1.9488 - val_accuracy: 0.6474\n",
      "Epoch 28/150\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 1.6379 - accuracy: 0.7590 - val_loss: 1.9152 - val_accuracy: 0.6474\n",
      "Epoch 29/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.6346 - accuracy: 0.7562 - val_loss: 1.8713 - val_accuracy: 0.6474\n",
      "Epoch 30/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.5360 - accuracy: 0.7562 - val_loss: 1.8423 - val_accuracy: 0.6474\n",
      "Epoch 31/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.5211 - accuracy: 0.7618 - val_loss: 1.8354 - val_accuracy: 0.6474\n",
      "Epoch 32/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.5183 - accuracy: 0.7618 - val_loss: 1.8290 - val_accuracy: 0.6474\n",
      "Epoch 33/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.5144 - accuracy: 0.7645 - val_loss: 1.8272 - val_accuracy: 0.6474\n",
      "Epoch 34/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.5104 - accuracy: 0.7618 - val_loss: 1.8221 - val_accuracy: 0.6474\n",
      "Epoch 35/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.5068 - accuracy: 0.7645 - val_loss: 1.8167 - val_accuracy: 0.6474\n",
      "Epoch 36/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.5037 - accuracy: 0.7618 - val_loss: 1.7384 - val_accuracy: 0.6603\n",
      "Epoch 37/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.5031 - accuracy: 0.7673 - val_loss: 1.7325 - val_accuracy: 0.6603\n",
      "Epoch 38/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 1.4985 - accuracy: 0.7645 - val_loss: 1.7262 - val_accuracy: 0.6538\n",
      "Epoch 39/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.4949 - accuracy: 0.7701 - val_loss: 1.7242 - val_accuracy: 0.6538\n",
      "Epoch 40/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 1.4919 - accuracy: 0.7701 - val_loss: 1.8767 - val_accuracy: 0.6667\n",
      "Epoch 41/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.4876 - accuracy: 0.7701 - val_loss: 1.8055 - val_accuracy: 0.6538\n",
      "Epoch 42/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.4850 - accuracy: 0.7701 - val_loss: 1.8052 - val_accuracy: 0.6667\n",
      "Epoch 43/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.4817 - accuracy: 0.7756 - val_loss: 1.8650 - val_accuracy: 0.6731\n",
      "Epoch 44/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.4775 - accuracy: 0.7729 - val_loss: 1.8647 - val_accuracy: 0.6795\n",
      "Epoch 45/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.4751 - accuracy: 0.7812 - val_loss: 1.8602 - val_accuracy: 0.6795\n",
      "Epoch 46/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.4714 - accuracy: 0.7812 - val_loss: 1.7917 - val_accuracy: 0.6795\n",
      "Epoch 47/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.4689 - accuracy: 0.7839 - val_loss: 1.8611 - val_accuracy: 0.6795\n",
      "Epoch 48/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.4654 - accuracy: 0.7895 - val_loss: 1.7842 - val_accuracy: 0.6667\n",
      "Epoch 49/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.4623 - accuracy: 0.7895 - val_loss: 1.7804 - val_accuracy: 0.6731\n",
      "Epoch 50/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.4589 - accuracy: 0.7922 - val_loss: 1.7697 - val_accuracy: 0.6603\n",
      "Epoch 51/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 1.4562 - accuracy: 0.7922 - val_loss: 1.7712 - val_accuracy: 0.6603\n",
      "Epoch 52/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.4534 - accuracy: 0.7950 - val_loss: 1.7700 - val_accuracy: 0.6603\n",
      "Epoch 53/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 1.4503 - accuracy: 0.7950 - val_loss: 1.7638 - val_accuracy: 0.6667\n",
      "Epoch 54/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.3865 - accuracy: 0.8089 - val_loss: 1.6448 - val_accuracy: 0.6474\n",
      "Epoch 55/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 1.3753 - accuracy: 0.8006 - val_loss: 1.6453 - val_accuracy: 0.6667\n",
      "Epoch 56/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.3679 - accuracy: 0.8006 - val_loss: 1.6422 - val_accuracy: 0.6731\n",
      "Epoch 57/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.3658 - accuracy: 0.8033 - val_loss: 1.6387 - val_accuracy: 0.6795\n",
      "Epoch 58/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.3602 - accuracy: 0.8061 - val_loss: 1.6345 - val_accuracy: 0.6795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 1.3568 - accuracy: 0.8033 - val_loss: 1.6338 - val_accuracy: 0.6795\n",
      "Epoch 60/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.3540 - accuracy: 0.8061 - val_loss: 1.6343 - val_accuracy: 0.6795\n",
      "Epoch 61/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.3519 - accuracy: 0.8089 - val_loss: 1.6334 - val_accuracy: 0.6859\n",
      "Epoch 62/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 1.3488 - accuracy: 0.8116 - val_loss: 1.6365 - val_accuracy: 0.6859\n",
      "Epoch 63/150\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 1.3454 - accuracy: 0.8089 - val_loss: 1.6443 - val_accuracy: 0.6923\n",
      "Epoch 64/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 1.3427 - accuracy: 0.8116 - val_loss: 1.7065 - val_accuracy: 0.6923\n",
      "Epoch 65/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 1.3410 - accuracy: 0.8089 - val_loss: 1.7071 - val_accuracy: 0.6923\n",
      "Epoch 66/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.3382 - accuracy: 0.8061 - val_loss: 1.7069 - val_accuracy: 0.6923\n",
      "Epoch 67/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.3350 - accuracy: 0.8116 - val_loss: 1.7075 - val_accuracy: 0.6923\n",
      "Epoch 68/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.3316 - accuracy: 0.8089 - val_loss: 1.7062 - val_accuracy: 0.6923\n",
      "Epoch 69/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 1.3289 - accuracy: 0.8116 - val_loss: 1.7177 - val_accuracy: 0.6923\n",
      "Epoch 70/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.3294 - accuracy: 0.8144 - val_loss: 1.7713 - val_accuracy: 0.6923\n",
      "Epoch 71/150\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 1.3256 - accuracy: 0.8172 - val_loss: 1.7712 - val_accuracy: 0.6923\n",
      "Epoch 72/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 1.3224 - accuracy: 0.8227 - val_loss: 1.7658 - val_accuracy: 0.6923\n",
      "Epoch 73/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 1.3186 - accuracy: 0.8255 - val_loss: 1.7692 - val_accuracy: 0.6987\n",
      "Epoch 74/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 1.3168 - accuracy: 0.8199 - val_loss: 1.7675 - val_accuracy: 0.6987\n",
      "Epoch 75/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.3167 - accuracy: 0.8199 - val_loss: 1.7640 - val_accuracy: 0.6987\n",
      "Epoch 76/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.3142 - accuracy: 0.8255 - val_loss: 1.7662 - val_accuracy: 0.7051\n",
      "Epoch 77/150\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 1.3101 - accuracy: 0.8227 - val_loss: 1.7654 - val_accuracy: 0.6987\n",
      "Epoch 78/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.3084 - accuracy: 0.8227 - val_loss: 1.7665 - val_accuracy: 0.6987\n",
      "Epoch 79/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.3069 - accuracy: 0.8227 - val_loss: 1.7655 - val_accuracy: 0.6987\n",
      "Epoch 80/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 1.3051 - accuracy: 0.8283 - val_loss: 1.7706 - val_accuracy: 0.7051\n",
      "Epoch 81/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.3060 - accuracy: 0.8255 - val_loss: 1.7680 - val_accuracy: 0.7115\n",
      "Epoch 82/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.2975 - accuracy: 0.8283 - val_loss: 1.7571 - val_accuracy: 0.7244\n",
      "Epoch 83/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.2968 - accuracy: 0.8310 - val_loss: 1.7619 - val_accuracy: 0.7244\n",
      "Epoch 84/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.2948 - accuracy: 0.8310 - val_loss: 1.7695 - val_accuracy: 0.7244\n",
      "Epoch 85/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.2926 - accuracy: 0.8310 - val_loss: 1.7123 - val_accuracy: 0.7179\n",
      "Epoch 86/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.2908 - accuracy: 0.8283 - val_loss: 1.7757 - val_accuracy: 0.7372\n",
      "Epoch 87/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.2883 - accuracy: 0.8338 - val_loss: 1.7022 - val_accuracy: 0.7244\n",
      "Epoch 88/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.2854 - accuracy: 0.8283 - val_loss: 1.6962 - val_accuracy: 0.7179\n",
      "Epoch 89/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.2834 - accuracy: 0.8366 - val_loss: 1.7056 - val_accuracy: 0.7051\n",
      "Epoch 90/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 1.2813 - accuracy: 0.8393 - val_loss: 1.6935 - val_accuracy: 0.7115\n",
      "Epoch 91/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.2806 - accuracy: 0.8393 - val_loss: 1.6962 - val_accuracy: 0.7051\n",
      "Epoch 92/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 1.2768 - accuracy: 0.8338 - val_loss: 1.7640 - val_accuracy: 0.7051\n",
      "Epoch 93/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 1.2741 - accuracy: 0.8449 - val_loss: 1.7715 - val_accuracy: 0.7115\n",
      "Epoch 94/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.2733 - accuracy: 0.8366 - val_loss: 1.7590 - val_accuracy: 0.6987\n",
      "Epoch 95/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.2723 - accuracy: 0.8449 - val_loss: 1.7661 - val_accuracy: 0.7051\n",
      "Epoch 96/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.2695 - accuracy: 0.8421 - val_loss: 1.6092 - val_accuracy: 0.6987\n",
      "Epoch 97/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.2699 - accuracy: 0.8449 - val_loss: 1.6805 - val_accuracy: 0.6987\n",
      "Epoch 98/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 1.2662 - accuracy: 0.8449 - val_loss: 1.6969 - val_accuracy: 0.7051\n",
      "Epoch 99/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.2724 - accuracy: 0.8421 - val_loss: 1.6149 - val_accuracy: 0.7051\n",
      "Epoch 100/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.2647 - accuracy: 0.8476 - val_loss: 1.6804 - val_accuracy: 0.7115\n",
      "Epoch 101/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.2576 - accuracy: 0.8532 - val_loss: 1.6814 - val_accuracy: 0.7179\n",
      "Epoch 102/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 1.2623 - accuracy: 0.8504 - val_loss: 1.6208 - val_accuracy: 0.7179\n",
      "Epoch 103/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.2237 - accuracy: 0.8421 - val_loss: 1.6174 - val_accuracy: 0.7115\n",
      "Epoch 104/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.1480 - accuracy: 0.8366 - val_loss: 1.4067 - val_accuracy: 0.7244\n",
      "Epoch 105/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.0616 - accuracy: 0.8476 - val_loss: 1.4844 - val_accuracy: 0.7308\n",
      "Epoch 106/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.0369 - accuracy: 0.8560 - val_loss: 1.3875 - val_accuracy: 0.7244\n",
      "Epoch 107/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 1.0251 - accuracy: 0.8560 - val_loss: 1.3733 - val_accuracy: 0.7436\n",
      "Epoch 108/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.0227 - accuracy: 0.8587 - val_loss: 1.3866 - val_accuracy: 0.7564\n",
      "Epoch 109/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 1.0142 - accuracy: 0.8643 - val_loss: 1.3652 - val_accuracy: 0.7692\n",
      "Epoch 110/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.0102 - accuracy: 0.8643 - val_loss: 1.3694 - val_accuracy: 0.7628\n",
      "Epoch 111/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 1.0082 - accuracy: 0.8643 - val_loss: 1.3669 - val_accuracy: 0.7628\n",
      "Epoch 112/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.0053 - accuracy: 0.8643 - val_loss: 1.4367 - val_accuracy: 0.7628\n",
      "Epoch 113/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1.0056 - accuracy: 0.8615 - val_loss: 1.4345 - val_accuracy: 0.7628\n",
      "Epoch 114/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.9994 - accuracy: 0.8643 - val_loss: 1.4441 - val_accuracy: 0.7628\n",
      "Epoch 115/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.9992 - accuracy: 0.8615 - val_loss: 1.5077 - val_accuracy: 0.7564\n",
      "Epoch 116/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 0s 3ms/step - loss: 0.9959 - accuracy: 0.8615 - val_loss: 1.5071 - val_accuracy: 0.7692\n",
      "Epoch 117/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.9938 - accuracy: 0.8670 - val_loss: 1.4436 - val_accuracy: 0.7692\n",
      "Epoch 118/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.9945 - accuracy: 0.8670 - val_loss: 1.5093 - val_accuracy: 0.7692\n",
      "Epoch 119/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.9891 - accuracy: 0.8726 - val_loss: 1.5145 - val_accuracy: 0.7628\n",
      "Epoch 120/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.9906 - accuracy: 0.8698 - val_loss: 1.5117 - val_accuracy: 0.7756\n",
      "Epoch 121/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.9866 - accuracy: 0.8670 - val_loss: 1.5128 - val_accuracy: 0.7628\n",
      "Epoch 122/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.9859 - accuracy: 0.8670 - val_loss: 1.5115 - val_accuracy: 0.7692\n",
      "Epoch 123/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.9836 - accuracy: 0.8698 - val_loss: 1.5207 - val_accuracy: 0.7564\n",
      "Epoch 124/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.9816 - accuracy: 0.8698 - val_loss: 1.5290 - val_accuracy: 0.7821\n",
      "Epoch 125/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.9813 - accuracy: 0.8643 - val_loss: 1.6062 - val_accuracy: 0.7692\n",
      "Epoch 126/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.9777 - accuracy: 0.8753 - val_loss: 1.5374 - val_accuracy: 0.7756\n",
      "Epoch 127/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.9780 - accuracy: 0.8726 - val_loss: 1.5202 - val_accuracy: 0.7692\n",
      "Epoch 128/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.9767 - accuracy: 0.8781 - val_loss: 1.5446 - val_accuracy: 0.7692\n",
      "Epoch 129/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.9750 - accuracy: 0.8643 - val_loss: 1.5258 - val_accuracy: 0.7692\n",
      "Epoch 130/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.9746 - accuracy: 0.8753 - val_loss: 1.5393 - val_accuracy: 0.7564\n",
      "Epoch 131/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.9694 - accuracy: 0.8781 - val_loss: 1.5361 - val_accuracy: 0.7564\n",
      "Epoch 132/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.9678 - accuracy: 0.8753 - val_loss: 1.5173 - val_accuracy: 0.7564\n",
      "Epoch 133/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.9664 - accuracy: 0.8809 - val_loss: 1.6149 - val_accuracy: 0.7500\n",
      "Epoch 134/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.9666 - accuracy: 0.8753 - val_loss: 1.5733 - val_accuracy: 0.7628\n",
      "Epoch 135/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.9624 - accuracy: 0.8837 - val_loss: 1.6258 - val_accuracy: 0.7628\n",
      "Epoch 136/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.9629 - accuracy: 0.8809 - val_loss: 1.5539 - val_accuracy: 0.7564\n",
      "Epoch 137/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.9587 - accuracy: 0.8837 - val_loss: 1.5539 - val_accuracy: 0.7564\n",
      "Epoch 138/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.9580 - accuracy: 0.8837 - val_loss: 1.6607 - val_accuracy: 0.7692\n",
      "Epoch 139/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.9579 - accuracy: 0.8809 - val_loss: 1.5452 - val_accuracy: 0.7756\n",
      "Epoch 140/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.9549 - accuracy: 0.8920 - val_loss: 1.5642 - val_accuracy: 0.7628\n",
      "Epoch 141/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.9531 - accuracy: 0.8837 - val_loss: 1.5875 - val_accuracy: 0.7564\n",
      "Epoch 142/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.9545 - accuracy: 0.8947 - val_loss: 1.7079 - val_accuracy: 0.7628\n",
      "Epoch 143/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.9510 - accuracy: 0.8947 - val_loss: 1.7744 - val_accuracy: 0.7628\n",
      "Epoch 144/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.9486 - accuracy: 0.9003 - val_loss: 1.7319 - val_accuracy: 0.7628\n",
      "Epoch 145/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.9482 - accuracy: 0.8892 - val_loss: 1.8646 - val_accuracy: 0.7500\n",
      "Epoch 146/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.9485 - accuracy: 0.8975 - val_loss: 1.7894 - val_accuracy: 0.7564\n",
      "Epoch 147/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.9466 - accuracy: 0.8947 - val_loss: 2.0235 - val_accuracy: 0.7692\n",
      "Epoch 148/150\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.9447 - accuracy: 0.8947 - val_loss: 2.0472 - val_accuracy: 0.7500\n",
      "Epoch 149/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.9427 - accuracy: 0.8920 - val_loss: 1.8046 - val_accuracy: 0.7628\n",
      "Epoch 150/150\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.9414 - accuracy: 0.9030 - val_loss: 1.8082 - val_accuracy: 0.7628\n"
     ]
    }
   ],
   "source": [
    "#Fit the model\n",
    "model.fit(X, Y, validation_split=0.3, epochs=150, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8bb84e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 2ms/step - loss: 1.1986 - accuracy: 0.8607\n",
      "accuracy: 86.07%\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the model\n",
    "scores = model.evaluate(X, Y)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d343a0a0",
   "metadata": {},
   "source": [
    "# Out of all iterations ,we are getting best accuracy with iteration-1,  94.58% ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c0d527",
   "metadata": {},
   "source": [
    "# Hence we can consider those combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765bddf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
